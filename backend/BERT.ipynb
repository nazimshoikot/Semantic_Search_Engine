{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the optimised model for creating vector embeddings\n",
    "import pickle\n",
    "\n",
    "# load the model\n",
    "bert_optimised = pickle.load(open(\"./../../BERT/roberta_optimised\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files read:  3027\n"
     ]
    }
   ],
   "source": [
    "# Reads the text documents from the directory, encodes the content into vectors\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# define directory\n",
    "directory = os.path.abspath('''./../../Dataset/Final dataset/all_documents_text''')\n",
    "\n",
    "# list to store content and names\n",
    "fileContents = []\n",
    "fileNames = [] \n",
    "\n",
    "# iterate through all files\n",
    "for file in os.listdir(directory):\n",
    "    # add file path to the fileNames array\n",
    "    fileNames.append(os.path.join(directory, file))\n",
    "\n",
    "    # read in the contents of the file\n",
    "    f = open(os.path.join(directory, file), \"r\", encoding=\"utf8\")\n",
    "    fileContent = f.read()\n",
    "    \n",
    "    # append to the contents array\n",
    "    fileContents.append(fileContent)\n",
    "    f.close()\n",
    "\n",
    "# check the numbers of files that have been read from thte direcotry\n",
    "print(\"Number of files read: \", len(fileNames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connents to the database, and inputs all the new documents \n",
    "# into the database so that they can be searched through the database\n",
    "# This section is only run when new documents are to be added to the database\n",
    "# Currently, there is no option of adding files from the front-end. We add it \n",
    "# manually from the backend. In the future, there can be an admin option that\n",
    "# calls this section to insert the new documents into the database\n",
    "\n",
    "import django\n",
    "import os.path\n",
    "\n",
    "# set the environment variables needed for access\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'rest.settings')\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "django.setup()\n",
    "\n",
    "from django.db.models import Max \n",
    "from docs.models import doc_information, doc_embedding\n",
    "\n",
    "# Finds the corresponding extension for the text file \n",
    "# specified after looking it up in the all_documents folder\n",
    "# which contains all the pdf documents\n",
    "# @name: name of the text file\n",
    "# returns the extension of the file if it can find it, or an empty string otherwise\n",
    "def findExtension(name):\n",
    "    doc_dir = \"./../../Dataset/Final dataset/all_documents\"\n",
    "    docs = os.listdir(doc_dir)\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_name = os.path.splitext(doc)[0]\n",
    "        ext_name = os.path.splitext(doc)[1]\n",
    "        if name == doc_name:\n",
    "            return ext_name\n",
    "    return \"\"\n",
    "\n",
    "new_dir = \"http://127.0.0.1:8080/\"\n",
    "\n",
    "\n",
    "# populates the database using the file content and information \n",
    "# received in the previou section. It checks if the file already\n",
    "# exists in the database and records the file if it doesn't already exist\n",
    "# new_dir is the address of server where the documents are\n",
    "# hosted and an be accessed\n",
    "def populateDatabase():\n",
    "    \n",
    "    # get all the current documents in the table\n",
    "    all_doc_information = doc_information.objects.all()\n",
    "    all_doc_embedding = doc_embedding.objects.all()\n",
    "\n",
    "    # gets the id of the last doc entered\n",
    "    last_id = all_doc_information.aggregate(Max('doc_id'))\n",
    "    last_id = last_id['doc_id__max']\n",
    "    if last_id == None:\n",
    "        last_id = 0\n",
    "    \n",
    "    # input the docs into the database\n",
    "    entryCount = 0\n",
    "    totalFound = 0 \n",
    "    for i in range(len(fileContents)):\n",
    "        foundCount = 0 # flag for checking if a document is already in the databasse\n",
    "\n",
    "        # extract the name of the file from the path\n",
    "        path_of_file = fileNames[i]\n",
    "        name_of_file = os.path.basename(path_of_file)\n",
    "        name_of_file = os.path.splitext(name_of_file)[0]\n",
    "\n",
    "        # find the content and create the preview\n",
    "        file_content = fileContents[i]\n",
    "        file_content = file_content.strip()\n",
    "        file_preview = file_content[:300] + \"...\"\n",
    "\n",
    "        # check whether the documents already exists\n",
    "        # set flag foundCount = 1 if it exists\n",
    "        for doc in all_doc_information:\n",
    "            if doc.document_name == name_of_file:\n",
    "                foundCount = 1\n",
    "                totalFound+= 1\n",
    "                print(\"Already Found: \", totalFound)\n",
    "                break\n",
    "\n",
    "        # if the document does not exist\n",
    "        if foundCount == 0:\n",
    "            \n",
    "            # find the path\n",
    "            new_path = new_dir + name_of_file + findExtension(name_of_file)\n",
    "            # add the doc to databasse\n",
    "            new_doc_information = doc_information(document_name = name_of_file, filepath=new_path, preview = file_preview)\n",
    "            new_doc_information.save()\n",
    "\n",
    "            # update the last id\n",
    "            last_id = last_id + 1\n",
    "\n",
    "            # set the size of the characters for each embedding\n",
    "            cutsize = 1000\n",
    "            # divide into sections of 1000 characters\n",
    "            count = 0\n",
    "            for i in range(len(file_content)//cutsize):\n",
    "                count += 1\n",
    "                content = file_content[i*cutsize:(i+1)*cutsize]\n",
    "                # embed the section of the content\n",
    "                embedding = bert_optimised.encode(content).tobytes()\n",
    "                new_doc_embedding = doc_embedding(doc_id = new_doc_information, embeddings = embedding)\n",
    "                new_doc_embedding.save()\n",
    "\n",
    "            # last section of the content\n",
    "            last_start = len(file_content) - len(file_content)%cutsize\n",
    "            last_content = file_content[last_start:]\n",
    "            count +=1\n",
    "            embedding = bert_optimised.encode(last_content).tobytes()\n",
    "            new_doc_embedding = doc_embedding(doc_id = new_doc_information, embeddings = embedding)\n",
    "            new_doc_embedding.save()\n",
    "            entryCount += 1\n",
    "            print(\"Entered: \", entryCount)\n",
    "            print(\"Number of embeddings: \", count)\n",
    "\n",
    "populateDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets individual embedded vectors, creates a np array of embedded vectors\n",
    "# and adds it to embedding_dictionary for easy access\n",
    "\n",
    "# define empty arrays to hold embeddings and filenames respectively\n",
    "extracted_embeddings = []\n",
    "fileNames = []\n",
    "\n",
    "# get all the documents information and embeddings\n",
    "all_doc_information = doc_information.objects.all()\n",
    "all_doc_embedding = doc_embedding.objects.all()\n",
    "\n",
    "# define empty dictionary to store information from database\n",
    "# for easy access\n",
    "embedding_dictionary = {}\n",
    "\n",
    "# for every document in database\n",
    "for doc in all_doc_information:\n",
    "    # get all the embeddings for that document\n",
    "    embedding_docs = all_doc_embedding.filter(doc_id=doc.doc_id)\n",
    "    temp_arr = []\n",
    "    for embedding in embedding_docs:\n",
    "        # covert the embedding from binary to np array of type float32\n",
    "        extracted_embedding = np.frombuffer(embedding.embeddings, dtype = \"float32\")\n",
    "        # add the embedding to the array of embeddings\n",
    "        temp_arr.append(extracted_embedding)\n",
    "        \n",
    "    # convert the whole embeddings array into an np array \n",
    "    # which is needed for comparison\n",
    "    temp_arr = np.array(temp_arr) \n",
    "    # add the array of embeddings for that document\n",
    "    embedding_dictionary[doc.doc_id] = temp_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is used for testing on the backend\n",
    "# it will not be executed during running the semantic search\n",
    "# rather, it emulates the functionality of the searching queries many different times\n",
    "# with different thresholds to find the optimal threshold for actual search\n",
    "\n",
    "import scipy\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# filters the distances in a given array according to a threshold\n",
    "# @arr: array to filter\n",
    "# @threshold: a threshold to find the arrays\n",
    "# returns all elements below the threshold, if the resulting array\n",
    "# is an empty array, returns the original array\n",
    "def filterDistances(arr, t):\n",
    "    new_arr = []\n",
    "    for element in arr:\n",
    "        if element < t:\n",
    "            new_arr.append(element)\n",
    "\n",
    "    if len(new_arr) == 0:\n",
    "        return arr\n",
    "    return new_arr\n",
    "\n",
    "# define query\n",
    "queries= [\"Machine learning developments in the industry\",\n",
    "         \"What are some of the cybersecurity risks faced by the company?\",\n",
    "         \"How likely are we to go into financial crisis?\",\n",
    "         \"Does technology improve company performance?\",\n",
    "         \"How to implement robotic process automation?\",\n",
    "         \"How to utilise machine learning?\",\n",
    "         \"What are the major risks of the company?\",\n",
    "         \"What was the financial performance?\",\n",
    "         \"What are the accounting practices of the company?\",\n",
    "         \"What are the new trends in artificial intelligence?\",\n",
    "         \"What is our business strategy?\"]\n",
    "\n",
    "# Find the closest 10 sentences of the corpus for each query sentence based on cosine similarity\n",
    "number_top_matches = 10 #@param {type: \"number\"}\n",
    "\n",
    "# show the results\n",
    "thresholds = np.linspace(0,1,101)\n",
    "\n",
    "averages_using_different_thresholds = []\n",
    "# @embedding_dictionary: map of every doc:embeddings\n",
    "for t in thresholds:\n",
    "    average_using_same_threshold_different_queries = []\n",
    "    for query in queries:\n",
    "        query_embedding = bert_optimised.encode(query)\n",
    "\n",
    "        results = [] # average distance of EVERY DOCUMENT from query\n",
    "\n",
    "        # finds the average distance of every document from query and add to results\n",
    "        # @doc: doc_id of every document\n",
    "        for doc in embedding_dictionary:\n",
    "            # extracted_embeddings: array of embeddings\n",
    "            extracted_embeddings = embedding_dictionary[doc]\n",
    "            \n",
    "            #distance of query from every embedding\n",
    "            # @distances: distance of EVERY EMBEDDING in ONE doc\n",
    "            distances = scipy.spatial.distance.cdist([query_embedding], extracted_embeddings, \"cosine\")[0]\n",
    "            distances = filterDistances(distances, t)\n",
    "\n",
    "            # @avg_distance: average of distances under the threshold\n",
    "            average_distance = sum(distances)/len(distances)\n",
    "\n",
    "            results.append(average_distance)\n",
    "\n",
    "\n",
    "        # sorted results of all documents for that query\n",
    "        results = sorted(results)\n",
    "        results = results[:10]\n",
    "        \n",
    "        # @result_average: the average result of top10 SEARCH RESULTS using \n",
    "        # that threshold for that query\n",
    "        result_average = sum(results)/len(results)\n",
    "        average_using_same_threshold_different_queries.append(result_average)\n",
    "\n",
    "    average_of_all_queries = sum(average_using_same_threshold_different_queries)/len(average_using_same_threshold_different_queries)\n",
    "    averages_using_different_thresholds.append(average_of_all_queries)\n",
    "\n",
    "min_threshold = thresholds[np.argmin(averages_using_different_thresholds)]\n",
    "print(\"Threshold: \", min_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# x axis values\n",
    "x = thresholds\n",
    "# corresponding y axis values\n",
    "y = averages_using_different_thresholds\n",
    "\n",
    "# plotting the points \n",
    "plt.plot(x, y)\n",
    "  \n",
    "# naming the x axis\n",
    "plt.xlabel('thresholds')\n",
    "# naming the y axis\n",
    "plt.ylabel('averages of top 10 results')\n",
    "  \n",
    "# giving a title to my graph\n",
    "plt.title('Average-Threshold graph')\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "\n",
    "# define query\n",
    "query = 'assessment of performance of Vodafone'\n",
    "query = query.strip()\n",
    "min_threshold = 0.51\n",
    "\n",
    "\n",
    "# filters the distances in a given array according to a threshold\n",
    "# @arr: array to filter\n",
    "# @threshold: a threshold to find the arrays\n",
    "# returns all elements below the threshold, if the resulting array\n",
    "# is an empty array, returns the original array\n",
    "def filterDistances(arr, t):\n",
    "    new_arr = []\n",
    "    for element in arr:\n",
    "        if element < t:\n",
    "            new_arr.append(element)\n",
    "\n",
    "    if len(new_arr) == 0:\n",
    "        return arr\n",
    "    return new_arr\n",
    "\n",
    "def run_query(query):\n",
    "\n",
    "    \n",
    "    query_embedding = bert_optimised.encode(query)\n",
    "\n",
    "    # Find the closest 50 sentences of the (for the test dataset)\n",
    "    number_top_matches = 50 #@param {type: \"number\"}\n",
    "\n",
    "    results = []\n",
    "    for doc in embedding_dictionary:\n",
    "        extracted_embeddings = embedding_dictionary[doc]\n",
    "        #distance of query from every embedding\n",
    "        distances = scipy.spatial.distance.cdist([query_embedding], extracted_embeddings, \"cosine\")[0]\n",
    "        distances = filterDistances(distances, min_threshold)\n",
    "\n",
    "        average_distance = sum(distances)/len(distances)\n",
    "\n",
    "        result = (doc, average_distance)\n",
    "        results.append(result)\n",
    "\n",
    "    # sort\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    response_docs = [query]\n",
    "    i = 1\n",
    "    for docid, distance in results[0:number_top_matches]:\n",
    "        doc = doc_information.objects.get(doc_id=docid)\n",
    "        response_doc = (doc.document_name)\n",
    "\n",
    "        response_docs.append(response_doc)\n",
    "        i += 1\n",
    "\n",
    "    return response_docs\n",
    "\n",
    "run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\nazim\\STUDIES\\HKU\\FYP\\confusion_matrix\\data2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "# reads the queries from the excel file\n",
    "directory_excel = os.path.abspath('''./../confusion_matrix/data2.xlsx''')\n",
    "queries = pd.read_excel(directory_excel, index_col=0)\n",
    "\n",
    "# runs the queries, adding it to all_results\n",
    "all_results = []\n",
    "for query, row in queries.iterrows():\n",
    "    result = run_query(query)\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes all results the query_results\n",
    "df = pd.DataFrame(all_results).T\n",
    "directory_excel = os.path.abspath('''./../confusion_matrix/query_results.xlsx''')\n",
    "df.to_excel(excel_writer = directory_excel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
